# voice/speech_to_text.py
import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel

print("âœ… speech_to_text.py loaded")

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆç¬¬ä¸€æ¬¡æœƒæ…¢ä¸€é»ï¼‰
model = WhisperModel(
    "base",
    device="cpu",
    compute_type="int8"
)

SAMPLE_RATE = 16000
RECORD_SECONDS = 5


def listen_and_transcribe():
    print("ğŸ¤ è«‹é–‹å§‹èªªè©±ï¼ˆ5 ç§’ï¼‰...")

    audio = sd.rec(
        int(RECORD_SECONDS * SAMPLE_RATE),
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype="float32"
    )
    sd.wait()

    audio = np.squeeze(audio)

    print("ğŸ§  è¾¨è­˜ä¸­...")

    segments, _ = model.transcribe(
        audio,
        language="zh",
        beam_size=5
    )

    text = "".join([seg.text for seg in segments]).strip()
    print(f"ğŸ“ ä½ èªªçš„æ˜¯ï¼š{text}")

    return text
