import os
import base64
import re
from typing import List, Dict, Optional, Union, Any

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

import google.genai as genai
from google.genai import types


# =========================
# Config
# =========================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()
ENABLE_WEB_SEARCH = os.getenv("ENABLE_WEB_SEARCH", "0").strip() == "1"  # æ¨¡å‹è‡ªå‹•åˆ¤æ–·éœ€è¦æ‰æœå°‹
MAX_MEMORY_MESSAGES = int(os.getenv("MAX_MEMORY_MESSAGES", "30"))
# =========================
# RAG Config
# =========================
RAG_FILE_SIZE_THRESHOLD = 1_000_000  # 1MB ä»¥ä¸Š PDF å•Ÿç”¨ RAG

# æ¨¡å‹å¯ä¾ä½ çš„é¡åº¦/éœ€æ±‚èª¿æ•´
MODEL_FAST = os.getenv("MODEL_FAST", "gemini-2.5-flash").strip()   # åœ–ç‰‡åˆ†é¡/æ‘˜è¦/ä¸€èˆ¬èŠå¤©
MODEL_TEXT = os.getenv("MODEL_TEXT", "gemini-2.5-flash").strip()   # ç´”æ–‡å­—èŠå¤©ï¼ˆå¯åŒ MODEL_FASTï¼‰

client = genai.Client(api_key=GEMINI_API_KEY) if GEMINI_API_KEY else None


# =========================
# FastAPI
# =========================
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],   # è‹¥ä½ è¦æ›´å®‰å…¨å¯æ”¹æˆä½ çš„å‰ç«¯ domain
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# =========================
# Response Model
# =========================
class ChatResponse(BaseModel):
    reply: str
class VoiceCommandRequest(BaseModel):
    text: str

class VoiceCommandResponse(BaseModel):
    reply: str
    need_confirm: bool = True
    action: Optional[str] = None
    slots: Optional[dict] = None


# =========================
# Memoryï¼ˆåœ–ç‰‡ + å°è©± + PDFï¼‰
# =========================
"""
chat_memory çµæ§‹ï¼š
[
  { "role": "user", "content": "å•é¡Œ" },
  { "role": "assistant", "content": "å›ç­”" },
  {
    "role": "file",
    "content": "ä½¿ç”¨è€…ä¸Šå‚³æª”æ¡ˆæ™‚çš„æ–‡å­—æè¿°",
    "filename": "...",
    "b64": "...",
    "mime": "image/jpeg" | "application/pdf",
    "summary": "æª”æ¡ˆæ‘˜è¦",
    "type": "calendar/table/document/ui/map/photo/other"
  }
]
"""
chat_memory: List[Dict[str, Any]] = []


def trim_memory():
    """é¿å…è¨˜æ†¶ç„¡é™è†¨è„¹ï¼ˆä¿ç•™æœ€å¾Œ MAX_MEMORY_MESSAGES ç­†ï¼‰"""
    global chat_memory
    if len(chat_memory) > MAX_MEMORY_MESSAGES:
        chat_memory = chat_memory[-MAX_MEMORY_MESSAGES:]


# =========================
# Prompts
# =========================
RAG_ANSWER_PROMPT = """
ä½ æ˜¯ä¸€å€‹æ–‡ä»¶å•ç­”åŠ©ç†ã€‚
è«‹åªæ ¹æ“šä»¥ä¸‹ã€Œæ–‡ä»¶ç‰‡æ®µã€å›ç­”å•é¡Œã€‚
- è‹¥æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè«‹æ˜ç¢ºå›ç­”ã€Œæ–‡ä»¶ä¸­æœªæåŠã€
- ä¸è¦è‡ªè¡Œæ¨æ¸¬
- å›ç­”è«‹ç°¡æ½”ã€æœ‰æ ¹æ“š
"""

IMAGE_ANALYZE_PROMPT = """
ä½ æ˜¯ä¸€å€‹åœ–ç‰‡ / PDF ç†è§£åŠ©ç†ã€‚
è«‹æ ¹æ“šè¼¸å…¥å…§å®¹ï¼Œå®Œæˆä»¥ä¸‹å…©ä»¶äº‹ï¼Œä¸¦ã€Œåªç”¨ JSON æ ¼å¼ã€å›ç­”ï¼š

{
  "type": "calendar | table | document | handwritten | ui | map | photo | other",
  "summary": "200 å­—ä»¥å…§çš„å…§å®¹æ‘˜è¦"
}

è¦å‰‡ï¼š
- type åªèƒ½æ˜¯åˆ—èˆ‰çš„å…¶ä¸­ä¸€å€‹è‹±æ–‡å€¼
- summary è«‹ä¾å…§å®¹å®¢è§€æ‘˜è¦
- ä¸è¦å›ç­”ä½¿ç”¨è€…å•é¡Œ
- ä¸è¦åŠ å…¥é¡å¤–èªªæ˜æ–‡å­—
"""


def build_file_answer_prompt(file_type: str) -> str:
    base = """
ä½ æ˜¯ä¸€å€‹ã€Œåœ–ç‰‡/æ–‡ä»¶ç†è§£åŠ©ç†ã€ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œï¼Œ
åªå›ç­”èˆ‡å•é¡Œç›´æ¥ç›¸é—œçš„è³‡è¨Šã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
"""
    if file_type == "calendar":
        base += """
âš ï¸ è¦å‰‡ï¼š
1. åƒ…å›ç­”å•é¡Œæ¶‰åŠçš„æ—¥æœŸã€ç¯€æ—¥ã€äº‹ä»¶æˆ–æ™‚æ®µã€‚
2. ä¸è¦åˆ—æ•´å€‹æœˆ/æ•´ä»½è¡Œäº‹æ›†ã€‚
3. è‹¥æ‰¾ä¸åˆ°ç²¾æº–ç­”æ¡ˆï¼Œè«‹å›ç­”ã€Œè¡Œäº‹æ›†ä¸­æ²’æœ‰æ‰¾åˆ°é€™å€‹è³‡è¨Šã€ã€‚
"""
    elif file_type == "table":
        base += """
âš ï¸ è¦å‰‡ï¼š
1. æ ¹æ“šæ‘˜è¦å›ç­”æ¬„ä½/æ•¸æ“šã€‚
2. ä¸è¦è¼¸å‡ºæ•´å¼µè¡¨æ ¼ã€‚
"""
    elif file_type == "document":
        base += """
âš ï¸ è¦å‰‡ï¼š
1. æ ¹æ“šæ‘˜è¦å…§å®¹å›ç­”ã€‚
2. å›ç­”å‹™å¿…ç°¡æ½”ã€æŠ“é‡é»ã€‚
"""
    else:
        base += """
âš ï¸ è¦å‰‡ï¼š
1. åƒ…ä¾æ‘˜è¦å…§å®¹å›ç­”ã€‚
2. è‹¥æ‘˜è¦æ²’æœ‰ï¼Œè«‹å›ç­”ã€Œæˆ‘åœ¨æª”æ¡ˆæ‘˜è¦ä¸­æ²’æœ‰çœ‹åˆ°ç›¸é—œè³‡è¨Šã€ã€‚
"""
    return base


# =========================
# Gemini helpers
# =========================
def make_part_from_bytes(data: bytes, mime: str) -> "types.Part":
    """
    ä¿®æ­£ä½ é‡åˆ°çš„ Part.from_bytes åƒæ•¸å·®ç•°å•é¡Œï¼š
    æœ‰äº›ç‰ˆæœ¬æ˜¯ from_bytes(data=..., mime_type=...)
    æœ‰äº›ç‰ˆæœ¬å…è¨± from_bytes(data, mime)
    """
    try:
        return types.Part.from_bytes(data=data, mime_type=mime)
    except TypeError:
        # fallbackï¼šèˆŠç‰ˆ/ä¸åŒç°½å
        return types.Part.from_bytes(data, mime)

def parse_voice_intent(text: str) -> dict:
    """
    é LLM çš„ä¿å®ˆç‰ˆè§£æï¼ˆä¸€å®šéï¼‰
    """
    if "å¯„ä¿¡" in text or "å¯«ä¿¡" in text:
        recipient = None

        if "ä¸»ç®¡" in text:
            recipient = "ä¸»ç®¡"
        elif "è€é—†" in text:
            recipient = "è€é—†"

        # è¶…ä¿å®ˆï¼šæŠŠã€Œå¯„ä¿¡çµ¦XXXã€å¾Œé¢çš„ç•¶å…§å®¹
        body = text
        body = re.sub(r"å¹«æˆ‘|è«‹|å¯„ä¿¡çµ¦.*?[,ï¼Œ]?", "", body).strip()

        return {
            "intent": "send_email",
            "slots": {
                "recipient": recipient,
                "body": body
            }
        }

    return {
        "intent": "chat",
        "slots": {}
    }

import time
import random

def safe_generate_content(
    *,
    model: str,
    contents: list,
    tools: Optional[list] = None,
    max_retry: int = 3
) -> str:
    """
    çµ±ä¸€è™•ç† Gemini å‘¼å«éŒ¯èª¤ï¼ˆ429 / 503 / overloadedï¼‰
    å«è‡ªå‹• retry + backoff
    """
    for attempt in range(max_retry):
        try:
            if tools:
                res = client.models.generate_content(
                    model=model,
                    contents=contents,
                    config=types.GenerateContentConfig(tools=tools),
                )
            else:
                res = client.models.generate_content(
                    model=model,
                    contents=contents,
                )
            return (res.text or "").strip()

        except Exception as e:
            msg = str(e)

            # ===== éè¼‰ / æš«æ™‚ä¸å¯ç”¨ï¼ˆæœ€å¸¸è¦‹ï¼‰=====
            if (
                "503" in msg
                or "UNAVAILABLE" in msg
                or "overloaded" in msg.lower()
            ):
                if attempt < max_retry - 1:
                    # æŒ‡æ•¸é€€é¿ + jitter
                    sleep_time = 1.2 * (attempt + 1) + random.uniform(0, 0.6)
                    time.sleep(sleep_time)
                    continue
                return "âš ï¸ æ¨¡å‹ç›®å‰ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆç³»çµ±å·²è‡ªå‹•é‡è©¦ï¼‰"

            # ===== é¡åº¦ç”¨ç›¡ =====
            if "429" in msg or "RESOURCE_EXHAUSTED" in msg or "quota" in msg.lower():
                return "âš ï¸ ç›®å‰ Gemini é¡åº¦æˆ–æµé‡å·²é”ä¸Šé™ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æ›´æ›æ¨¡å‹ã€‚"

            # ===== å…¶ä»–éŒ¯èª¤ =====
            return f"âš ï¸ Gemini å‘¼å«å¤±æ•—ï¼š{msg}"

# =========================
# RAG Helpers
# =========================
import numpy as np

rag_store = []  # æš«å­˜å‘é‡ï¼ˆå…ˆç”¨è¨˜æ†¶é«”ï¼Œä¸ç ´å£ç¾æœ‰æ¶æ§‹ï¼‰


def chunk_text(text: str, chunk_size=500, overlap=100):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks


def embed_texts(texts):
    res = client.models.embed_content(
        model="text-embedding-004",
        content=texts
    )
    return [e.values for e in res.embeddings]


def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def add_to_rag_store(chunks, embeddings):
    for c, e in zip(chunks, embeddings):
        rag_store.append({
            "text": c,
            "embedding": e
        })


def retrieve_relevant_chunks(query, top_k=3):
    query_emb = embed_texts([query])[0]
    scored = []
    for item in rag_store:
        score = cosine_similarity(query_emb, item["embedding"])
        scored.append((score, item["text"]))
    scored.sort(reverse=True)
    return [t for _, t in scored[:top_k]]


# =========================
# Web search (Gemini tool)
# =========================
def should_use_web_search(user_message: str) -> bool:
    """
    ç¬¬ä¸€å±¤ï¼šå¿«é€Ÿ heuristicï¼Œé¿å…æ¯æ¬¡éƒ½è§¸ç™¼å·¥å…·ï¼ˆçœé¡åº¦ã€åŠ å¿«é€Ÿåº¦ï¼‰ã€‚
    çœŸæ­£ã€Œæ¨¡å‹è‡ªå‹•åˆ¤æ–·ã€æœƒåœ¨ç¬¬äºŒå±¤ prompt å†æ±ºç­–ã€‚
    """
    triggers = [
        "æœ€æ–°", "æ–°è", "ä»Šå¤©", "ç¾åœ¨", "åƒ¹æ ¼", "è©•åƒ¹", "å“ªè£¡è²·", "ç¶­åŸº",
        "è¦æ ¼", "ä¸Šå¸‚", "ç™¼è¡¨", "èª°æ˜¯", "æ˜¯ä»€éº¼", "ç‚ºä»€éº¼å¤§å®¶èªª", "æ¯”è¼ƒ",
        "VW", "golf", "BMW", "i4", "VAG"
    ]
    return any(t.lower() in user_message.lower() for t in triggers)


def web_search_tools() -> list:
    """
    Gemini å®˜æ–¹ Web Search Toolï¼ˆgoogle_searchï¼‰ã€‚
    æ³¨æ„ï¼šéœ€è¦ä½ çš„å°ˆæ¡ˆ/å¸³è™Ÿæ”¯æ´è©² toolï¼Œå¦å‰‡æœƒå ±éŒ¯ã€‚
    """
    try:
        return [types.Tool(google_search=types.GoogleSearch())]
    except Exception:
        # å¦‚æœä½ çš„ google-genai ç‰ˆæœ¬æ²’æœ‰ GoogleSearch é¡åˆ¥ï¼Œå°±å…ˆä¸å•Ÿç”¨
        return []


# =========================
# Health Check
# =========================
@app.get("/")
async def root():
    return {"status": "ok", "message": "Gemini multimodal assistant running"}


# =========================
# (Optional) Reset memory
# =========================
@app.post("/reset", response_model=ChatResponse)
async def reset():
    chat_memory.clear()
    return ChatResponse(reply="âœ… å·²æ¸…é™¤å¾Œç«¯è¨˜æ†¶ï¼ˆchat_memoryï¼‰")


# =========================
# Chat API (multipart: message + image/pdf)
# =========================
@app.post("/chat", response_model=ChatResponse)
async def chat(
    message: str = Form(""),
    image: Optional[UploadFile] = File(None),
):
    if not client:
        return ChatResponse(reply="âŒ å¾Œç«¯å°šæœªè¨­å®š GEMINI_API_KEY")

    message = (message or "").strip()

    # =========================
    # Case 1ï¼šæœ‰ä¸Šå‚³æª”æ¡ˆï¼ˆåœ–ç‰‡æˆ– PDFï¼‰
    # =========================
    if image is not None:
        file_bytes = await image.read()
        mime = (image.content_type or "").strip().lower() or "application/octet-stream"

        use_rag = (
            mime == "application/pdf"
            and len(file_bytes) > RAG_FILE_SIZE_THRESHOLD
        )

        file_b64 = base64.b64encode(file_bytes).decode("utf-8")
        part = make_part_from_bytes(file_bytes, mime)

        # (A+B) åˆ†æåœ–ç‰‡ / PDFï¼ˆä¸€æ¬¡ Gemini å‘¼å«ï¼‰
        import json
        result = safe_generate_content(
            model=MODEL_FAST,
            contents=[IMAGE_ANALYZE_PROMPT, part],
        )

        try:
            parsed = json.loads(result)
            file_type = parsed.get("type", "other").lower()
            file_summary = parsed.get("summary", "")
        except Exception:
            file_type = "other"
            file_summary = result

        # (C) å­˜è¨˜æ†¶
        chat_memory.append({
            "role": "file",
            "content": message or "[ä½¿ç”¨è€…ä¸Šå‚³æª”æ¡ˆ]",
            "filename": image.filename or "uploaded",
            "b64": file_b64,
            "mime": mime,
            "summary": file_summary,
            "type": file_type,
        })
        trim_memory()

        # (D) è‹¥æœ‰æå• â†’ å›ç­”
        if message:
            if use_rag:
                chunks = chunk_text(file_summary)
                embeddings = embed_texts(chunks)
                add_to_rag_store(chunks, embeddings)
                relevant = retrieve_relevant_chunks(message)

                reply = safe_generate_content(
                    model=MODEL_TEXT,
                    contents=[
                        RAG_ANSWER_PROMPT,
                        "ã€æ–‡ä»¶ç‰‡æ®µã€‘\n" + "\n---\n".join(relevant),
                        f"ä½¿ç”¨è€…å•é¡Œï¼š{message}"
                    ]
                )
            else:
                prompt = build_file_answer_prompt(file_type)
                convo = [
                    prompt,
                    f"ã€æª”æ¡ˆæ‘˜è¦ã€‘ï¼š{file_summary}",
                    f"ä½¿ç”¨è€…å•é¡Œï¼š{message}"
                ]
                reply = safe_generate_content(
                    model=MODEL_FAST,
                    contents=convo,
                )

            chat_memory.append({"role": "assistant", "content": reply})
            trim_memory()
            return ChatResponse(reply=reply)

        # æ²’æœ‰æå• â†’ å›æ‘˜è¦
        return ChatResponse(
            reply=f"âœ… å·²æ”¶åˆ°æª”æ¡ˆï¼ˆ{image.filename}ï¼‰ã€‚æˆ‘æ•´ç†çš„æ‘˜è¦å¦‚ä¸‹ï¼š\n\n{file_summary}"
        )

    # =========================
    # Case 2ï¼šç´”æ–‡å­—èŠå¤©
    # =========================
    if not message:
        return ChatResponse(reply="è«‹è¼¸å…¥å•é¡Œæˆ–ä¸Šå‚³åœ–ç‰‡ / PDF å–”ï¼")

    chat_memory.append({"role": "user", "content": message})
    trim_memory()

    system = """
ä½ æ˜¯ä¸€å€‹è‡ªç„¶ã€å£èªåŒ–ã€ä½†å›ç­”æœ‰é‡é»çš„ AI åŠ©ç†ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
ä½ æœƒçœ‹åˆ°ä¸€æ®µå°è©±è¨˜æ†¶èˆ‡ï¼ˆå¯èƒ½çš„ï¼‰æª”æ¡ˆæ‘˜è¦ã€‚
"""

    convo = [system]
    for m in chat_memory[-10:]:
        if m["role"] == "user":
            convo.append(f"ä½¿ç”¨è€…ï¼š{m['content']}")
        elif m["role"] == "assistant":
            convo.append(f"åŠ©ç†ï¼š{m['content']}")
        elif m["role"] == "file":
            convo.append(f"ï¼ˆå…ˆå‰æª”æ¡ˆæ‘˜è¦ï¼‰ï¼š{m.get('summary','')}")

    convo.append(f"ä½¿ç”¨è€…ï¼š{message}")

    tools = None
    if ENABLE_WEB_SEARCH and should_use_web_search(message):
        tools = web_search_tools() or None

    reply = safe_generate_content(
        model=MODEL_TEXT,
        contents=convo,
        tools=tools,
    )

    chat_memory.append({"role": "assistant", "content": reply})
    trim_memory()
    return ChatResponse(reply=reply)

# =========================
# Shared Core (Text-only)
# çµ¦ CLI / Voice Agent / æœªä¾† WebSocket ç”¨
# ä¸å½±éŸ¿ç¾æœ‰ /chat API
# =========================
def handle_text_query(message: str) -> str:
    """
    ç´”æ–‡å­—æŸ¥è©¢å…¥å£ï¼ˆä¸å«åœ–ç‰‡ / PDFï¼‰
    - å…±ç”¨æ—¢æœ‰ chat_memory
    - å…±ç”¨ Gemini è¨­å®šã€å·¥å…·ã€web search
    - ä¸å‹• FastAPI /chat è¡Œç‚º
    """

    if not client:
        return "âŒ å¾Œç«¯å°šæœªè¨­å®š GEMINI_API_KEY"

    message = (message or "").strip()
    if not message:
        return "è«‹è¼¸å…¥å•é¡Œå–”ï¼"

    # ---- è¨˜æ†¶ï¼šuser ----
    chat_memory.append({"role": "user", "content": message})
    trim_memory()

    system = """
ä½ æ˜¯ä¸€å€‹è‡ªç„¶ã€å£èªåŒ–ã€ä½†å›ç­”æœ‰é‡é»çš„ AI åŠ©ç†ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
"""

    convo = [system]

    # ---- æœ€è¿‘å°è©±è¨˜æ†¶ ----
    for m in chat_memory[-10:]:
        if m["role"] == "user":
            convo.append(f"ä½¿ç”¨è€…ï¼š{m['content']}")
        elif m["role"] == "assistant":
            convo.append(f"åŠ©ç†ï¼š{m['content']}")
        elif m["role"] == "file":
            convo.append(f"ï¼ˆå…ˆå‰æª”æ¡ˆæ‘˜è¦ï¼‰ï¼š{m.get('summary','')}")

    convo.append(f"ä½¿ç”¨è€…ï¼š{message}")

    tools = None
    if ENABLE_WEB_SEARCH and should_use_web_search(message):
        tools = web_search_tools() or None

    reply = safe_generate_content(
        model=MODEL_TEXT,
        contents=convo,
        tools=tools,
    )

    # ---- è¨˜æ†¶ï¼šassistant ----
    chat_memory.append({"role": "assistant", "content": reply})
    trim_memory()

    return reply

    from backend.voice_api import router as voice_router
    app.include_router(voice_router)

@app.post("/voice-command", response_model=VoiceCommandResponse)
async def voice_command(req: VoiceCommandRequest):
    text = (req.text or "").strip()
    if not text:
        return VoiceCommandResponse(
            reply="æˆ‘æ²’æœ‰è½æ¸…æ¥šï¼Œå¯ä»¥å†èªªä¸€æ¬¡å—ï¼Ÿ",
            need_confirm=False
        )

    intent_data = parse_voice_intent(text)
    intent = intent_data["intent"]
    slots = intent_data.get("slots", {})

    # ===== å¯„ä¿¡æµç¨‹ï¼ˆç¢ºèªéšæ®µï¼‰=====
    if intent == "send_email":
        recipient = slots.get("recipient") or "å°æ–¹"
        body = slots.get("body") or ""

        reply = (
            f"ä½ è¦å¯„ä¿¡çµ¦ã€Œ{recipient}ã€ï¼Œ"
            f"å…§å®¹æ˜¯ã€Œ{body}ã€ï¼Œå°å—ï¼Ÿ"
        )

        return VoiceCommandResponse(
            reply=reply,
            need_confirm=True,
            action="send_email",
            slots=slots
        )

    # ===== ä¸æ˜¯å‹•ä½œ â†’ ç•¶ä¸€èˆ¬èŠå¤© =====
    reply = handle_text_query(text)
    return VoiceCommandResponse(
        reply=reply,
        need_confirm=False
    )

class VoiceConfirmRequest(BaseModel):
    action: str
    slots: dict

class VoiceConfirmResponse(BaseModel):
    reply: str

from actions.send_email import send_email_via_outlook

CONTACTS = {
    "ä¸»ç®¡": "boss@example.com",
    "è€é—†": "boss@example.com",
}

@app.post("/voice-confirm", response_model=VoiceConfirmResponse)
async def voice_confirm(req: VoiceConfirmRequest):
    action = req.action
    slots = req.slots or {}

    # ===== å¯„ä¿¡ =====
    if action == "send_email":
        recipient_name = slots.get("recipient")
        body = slots.get("body", "")

        if not recipient_name:
            return VoiceConfirmResponse(
                reply="âŒ æ‰¾ä¸åˆ°æ”¶ä»¶äººï¼Œå·²å–æ¶ˆæ“ä½œ"
            )

        recipient_email = CONTACTS.get(recipient_name)
        if not recipient_email:
            return VoiceConfirmResponse(
                reply=f"âŒ æˆ‘ä¸çŸ¥é“ã€Œ{recipient_name}ã€æ˜¯èª°"
            )

        try:
            send_email_via_outlook(recipient_email, body)
            return VoiceConfirmResponse(
                reply=f"âœ… å·²å¹«ä½ å¯„ä¿¡çµ¦ã€Œ{recipient_name}ã€"
            )
        except Exception as e:
            return VoiceConfirmResponse(
                reply=f"âŒ å¯„ä¿¡å¤±æ•—ï¼š{str(e)}"
            )

    return VoiceConfirmResponse(
        reply="ğŸ¤· é€™å€‹æ“ä½œæˆ‘é‚„ä¸æœƒ"
    )



